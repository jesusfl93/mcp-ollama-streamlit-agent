
# ğŸ§  MCP + Ollama Streamlit Chatbot

This repository contains a fully functional multi-agent chatbot powered by the **Model Context Protocol (MCP)**, **Ollama** with the `qwen3:1.7b` model, and a **Streamlit-based frontend**. The chatbot supports tool calling and integrates domain-specific utilities like weather APIs, math evaluation, and CSV dataset analysis.

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ app.py             # Streamlit frontend interface
â”œâ”€â”€ client.py          # Async MCP client with Ollama integration and tool handling
â”œâ”€â”€ server.py          # MCP-compatible server with weather, math, and dataset tools
â”œâ”€â”€ data/
â”‚   â””â”€â”€ dataset.csv    # Sample CSV file for dataset analysis
â”œâ”€â”€ .env               # Environment variables (MCP_SSE_URL, etc.)
```

---

## ğŸ§° Features & Tools

The assistant supports the following built-in tools via MCP server:

- ğŸŒ¤ï¸ **Weather Tools** â€“ Fetch alerts and forecasts using the [National Weather Service API](https://www.weather.gov/documentation/services-web-api)
- â— **Math Evaluator** â€“ Safe evaluation of arithmetic expressions
- ğŸ“Š **Dataset Inspector** â€“ Summary statistics, shape, and NLP-powered queries on a local `dataset.csv` file

---

## ğŸ”§ Requirements

Ensure you have the following installed:

- Python 3.9+
- Ollama with the `qwen3:1.7b` model available locally
- MCP library (see installation below)
- [Streamlit](https://streamlit.io)
- [Uvicorn](https://www.uvicorn.org/) for ASGI server
- A `.env` file with MCP server URL defined:
  ```env
  MCP_SSE_URL=http://localhost:8080/sse
  ```

### Python Packages

You can install all required packages via:

```bash
pip install -r requirements.txt
```

If you don't have a `requirements.txt`, use:

```bash
pip install streamlit uvicorn httpx python-dotenv pandas scikit-learn mcp
```

---

## â–¶ï¸ How to Run

### 1. Launch the MCP Server (Tool Provider)

Run the server to expose SSE-compatible endpoints:

```bash
python server.py --host 0.0.0.0 --port 8080
```

This will start a FastAPI-compatible MCP server exposing tools on:
```
http://localhost:8080/sse
```

### 2. Start the Streamlit Frontend

In another terminal:

```bash
streamlit run app.py
```

This will open the chat interface in your browser at:
```
http://localhost:8501
```

---

## ğŸ“¦ Ollama Model Setup

Install and run Ollama:

```bash
ollama pull qwen3:1.7b
ollama run qwen3:1.7b
```

Ensure the model is loaded and responding at:
```
http://localhost:11434/api/chat
```

---

## ğŸ§ª Supported Use Cases

Here are some queries you can try:

### Weather
- *Whatâ€™s the weather in San Francisco?*
- *Are there any alerts in NY?*

### Math
- *What is 2 + 3 * 4?*
- *Calculate the square root of 81*

### Dataset Analysis
- *What are the columns in the dataset?*
- *How many records are in the file?*
- *Do any descriptions mention "cloud" or "AI"?*

---

## ğŸ§¼ Resetting State

You can reset the chat history using the sidebar button in the Streamlit UI.

---

## ğŸ“ Notes

- Make sure `data/dataset.csv` exists if using dataset tools.
- Ensure `MCP_SSE_URL` in `.env` matches your server setup.
- This system uses `asyncio.run_coroutine_threadsafe()` to allow asynchronous tool execution within Streamlit's synchronous model.